{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":64148,"databundleVersionId":7669720,"sourceType":"competition"},{"sourceId":168178971,"sourceType":"kernelVersion"},{"sourceId":10417,"sourceType":"modelInstanceVersion","modelInstanceId":8385,"modelId":3301},{"sourceId":11264,"sourceType":"modelInstanceVersion","modelInstanceId":8318,"modelId":3301}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Science AI Assistant with Gemma 2b-it","metadata":{}},{"cell_type":"markdown","source":"For the Kaggle competition **\"Google â€“ AI Assistants for Data Tasks with Gemma,\"** I've prepared this AI Assistant that can accomplish the task of **\"Explaining or teaching basic data science concepts,\"** as required by the competition.\n\nThis project is also an occasion to explain how a basic **retrieval-augmented generation (RAG)** system works, by showing the role of the data, which constitutes the backbone of the system, the function of embeddings and distance measures, how to retrieve relevant information for the task of answering a question, and how to process such information by first using a distillation prompt and then assembling the answer required by the user in a meaningful and useful way.\n\nIn this project, the lion's share is done by **Gemma**, the state-of-the-art open LLM model released by Google, in its <U>2b-it implementation, the smallest in terms of parameters</U>. Gemma is not the only Google technology presented in the project because I also make use of **ScaNN** ([ScaNN Github repository](https://github.com/google-research/google-research/tree/master/scann)) for recalling the information. Apart from Gemma, ScaNN, and HuggingFace packages for transformers and embeddings, there are no ready-made solutions such as vector stores or RAG packages. You can actually see how everything works under the hood, and if you like it, reuse it for your own projects.\n\n","metadata":{}},{"cell_type":"markdown","source":"![AI with Gemma](https://th.bing.com/th/id/OIG2.MHcSMMiDt6p95N.mjds0?pid=ImgGn)","metadata":{}},{"cell_type":"markdown","source":"# 1. What is a RAG and how it can help to explain or teach basic data science concepts","metadata":{}},{"cell_type":"markdown","source":"A **Retrieval-Augmented Generation (RAG)** is a solution that improves text generation of a large language model by integrating its answers using some kind of external knowledge retrieval.\n\nHence, it combines a **retriever** to fetch relevant information and a **generator** to produce accurate responses based on this retrieved knowledge. Basically, it is just like first doing a search engine query (the retriever), getting the best answers, and then asking a large language model such as **Gemma** or **Gemini** to process the information (generator) to answer an initial question.","metadata":{}},{"cell_type":"markdown","source":"![High-level RAG architecture](https://raw.githubusercontent.com/lmassaron/useful_stuff/main/High-Level%20RAG%20Architecture_rev2.jpg)","metadata":{}},{"cell_type":"markdown","source":"Such an approach ensures AI models have access to up-to-date and relevant facts, improving the quality and reliability of their generated text, especially in tasks like question-answering where factual accuracy is crucial and LLMs are infamous for sometimes coming up with made-up information (hallucinations).\n\nIn this case, **Google Gemma** seems already quite apt at answering basic questions about data science, but the idea is to further improve its competencies by providing it reliable information about AI, statistics, machine learning, and data science in general.","metadata":{}},{"cell_type":"markdown","source":"# 2. Setting up the necessary stuff","metadata":{}},{"cell_type":"markdown","source":"In the first cell of this notebook, some key packages for the project are installed or updated to the latest version:\n\n1. The first command installs or upgrades the **torch** package quietly, specifying version compatibility for CUDA 11.7 from the PyTorch repository.\n2. The second command installs or upgrades the **transformers** package to version **4.38.2**, a popular library for natural language processing tasks.\n3. The third command installs the **accelerate** package, which is used for optimizing machine learning training pipelines.\n4. The fourth command installs the **bitsandbytes** package from the specified index URL, potentially a custom or private package repository.\n5. The fifth command installs or upgrades the **sentence_transformers** package, known for providing pre-trained models for sentence embeddings.\n6. The sixth command installs or upgrades the **scann** package, likely used for approximate nearest neighbor search implementations.\n7. The seventh command installs or upgrades the **wikipedia-api** package, which provides an interface to interact with Wikipedia data programmatically.\n","metadata":{}},{"cell_type":"code","source":"!pip install -q -U torch --index-url https://download.pytorch.org/whl/cu117\n!pip install -q -U transformers==\"4.38.2\"\n!pip install -q accelerate\n!pip install -q -i https://pypi.org/simple/ bitsandbytes\n!pip install -q -U sentence_transformers\n!pip install -q -U scann\n!pip install -q -U wikipedia-api","metadata":{"execution":{"iopub.status.busy":"2024-09-27T01:10:13.112001Z","iopub.execute_input":"2024-09-27T01:10:13.112635Z","iopub.status.idle":"2024-09-27T01:12:57.683809Z","shell.execute_reply.started":"2024-09-27T01:10:13.112601Z","shell.execute_reply":"2024-09-27T01:12:57.682547Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ntensorflow-decision-forests 1.8.1 requires tensorflow~=2.15.0, but you have tensorflow 2.17.0 which is incompatible.\ntensorflow-text 2.15.0 requires tensorflow<2.16,>=2.15.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.17.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"In the next cell, the code uses the **os** module to set some environment variables. The first line sets the **CUDA_VISIBLE_DEVICES** variable to \"0,\" which instructs CUDA-enabled applications to use only the GPU with index 0 for computation, useful for managing GPU resources in multi-GPU systems. The second line sets **TOKENIZERS_PARALLELISM** to \"false,\" disabling parallelism in the Hugging Face Tokenizers library, potentially useful for troubleshooting or ensuring single-threaded execution. These environment variable configurations help control GPU usage and tokenizer behavior within the Python environment where this code is executed.","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-09-27T01:12:57.685708Z","iopub.execute_input":"2024-09-27T01:12:57.686011Z","iopub.status.idle":"2024-09-27T01:12:57.690785Z","shell.execute_reply.started":"2024-09-27T01:12:57.685984Z","shell.execute_reply":"2024-09-27T01:12:57.689897Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Moreover, since warnings may occur when using new versions of Python packages (aligning versions is often a task in itself), the following cell imports the **warnings** package and suppresses warnings during this session.","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-09-27T01:12:57.691856Z","iopub.execute_input":"2024-09-27T01:12:57.692086Z","iopub.status.idle":"2024-09-27T01:12:57.703146Z","shell.execute_reply.started":"2024-09-27T01:12:57.692065Z","shell.execute_reply":"2024-09-27T01:12:57.702311Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"In the next cell, the notebook loads Python libraries and modules for natural language processing tasks. It also includes libraries like **re** for regular expressions, **numpy** and **pandas** for data manipulation, **tqdm** for progress bars, **scann** for approximate nearest neighbor search, and **wikipediaapi** for accessing Wikipedia content (yes, we are going to use Wikipedia as a knowledge base).\n","metadata":{}},{"cell_type":"code","source":"import re\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport scann\nimport wikipediaapi\n\nimport torch\n\nimport transformers\nfrom transformers import (AutoModelForCausalLM, \n                          AutoTokenizer, \n                          BitsAndBytesConfig,\n                         )\nfrom sentence_transformers import SentenceTransformer\nimport bitsandbytes as bnb","metadata":{"execution":{"iopub.status.busy":"2024-09-27T01:12:57.705072Z","iopub.execute_input":"2024-09-27T01:12:57.705334Z","iopub.status.idle":"2024-09-27T01:13:09.796740Z","shell.execute_reply.started":"2024-09-27T01:12:57.705313Z","shell.execute_reply":"2024-09-27T01:13:09.795952Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-09-27 01:12:58.369995: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-09-27 01:12:58.391462: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-09-27 01:12:58.397826: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 3. Proceeding by building blocks","metadata":{}},{"cell_type":"markdown","source":"Before proceeding with the notebook, it is necessary to spend a word about how I will proceed in building the solution in a way that can be clear, easily explainable, and both reusable as well as hackable.","metadata":{}},{"cell_type":"markdown","source":"The AI assistant will simply be a class containing all you need for it to work and with methods for changing some settings (such as the temperature, which corresponds to its creativity, or the impersonated role, which influences how it responds) and for asking questions.\n\nAll the internal functions, however, are external, hence they are easier to present as stand-alone code snippets, easily reusable for different purposes or projects, and easily upgradable or hackable. Because as you change an external function, you immediately change the behavior of the class, without having to reinstantiate it again (it actually takes some time to re-index all the knowledge base, which may prevent some fast experimentation).\n","metadata":{}},{"cell_type":"markdown","source":"Here, as a first piece of code, the next cell presents a function that returns the device where to map the model and the data when working with the PyTorch library (used by the HF packages). It works with a **CPU-based** computer, a **GPU** one, and with a **macOS with MPS**.","metadata":{}},{"cell_type":"code","source":"def define_device():\n    \"\"\"Define the device to be used by PyTorch\"\"\"\n\n    # Get the PyTorch version\n    torch_version = torch.__version__\n\n    # Print the PyTorch version\n    print(f\"PyTorch version: {torch_version}\", end=\" -- \")\n\n    # Check if MPS (Multi-Process Service) device is available on MacOS\n    if torch.backends.mps.is_available():\n        # If MPS is available, print a message indicating its usage\n        print(\"using MPS device on MacOS\")\n        # Define the device as MPS\n        defined_device = torch.device(\"mps\")\n    else:\n        # If MPS is not available, determine the device based on GPU availability\n        defined_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        # Print a message indicating the selected device\n        print(f\"using {defined_device}\")\n\n    # Return the defined device\n    return defined_device\n","metadata":{"execution":{"iopub.status.busy":"2024-09-27T01:13:09.797896Z","iopub.execute_input":"2024-09-27T01:13:09.798413Z","iopub.status.idle":"2024-09-27T01:13:09.805032Z","shell.execute_reply.started":"2024-09-27T01:13:09.798389Z","shell.execute_reply":"2024-09-27T01:13:09.804010Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"The next cells, instead, present two functions designed to operate using the **SentenceTransformers** package ([the package home page](https://www.sbert.net/index.html)), that can operate with lists of text and map them into embeddings.\n","metadata":{}},{"cell_type":"markdown","source":"Embeddings, such as those processed by packages like **SentenceTransformers**, are representations of text or sentences in a numerical form that capture their semantic meaning. These embeddings are created by transforming words or sentences into high-dimensional vectors, where similar vectors represent similar meanings.","metadata":{}},{"cell_type":"markdown","source":"In the context of **SentenceTransformers**, these embeddings are generated using models like BERT or XLNet that have been fine-tuned to produce meaningful sentence representations. These embeddings can be used for various tasks like clustering, semantic textual similarity, and information retrieval (in our project we actually need a retrieval function) by comparing the vectors using metrics like cosine similarity.","metadata":{}},{"cell_type":"code","source":"def get_embedding(text, embedding_model):\n    \"\"\"Get embeddings for a given text using the provided embedding model\"\"\"\n    \n    # Encode the text to obtain embeddings using the provided embedding model\n    embedding = embedding_model.encode(text, show_progress_bar=False)\n    \n    # Convert the embeddings to a list of floats and return\n    return embedding.tolist()\n\ndef map2embeddings(data, embedding_model):\n    \"\"\"Map a list of texts to their embeddings using the provided embedding model\"\"\"\n    \n    # Initialize an empty list to store embeddings\n    embeddings = []\n\n    # Iterate over each text in the input data list\n    no_texts = len(data)\n    print(f\"Mapping {no_texts} pieces of information\")\n    for i in tqdm(range(no_texts)):\n        # Get embeddings for the current text using the provided embedding model\n        embeddings.append(get_embedding(data[i], embedding_model))\n    \n    # Return the list of embeddings\n    return embeddings","metadata":{"execution":{"iopub.status.busy":"2024-09-27T01:13:09.806097Z","iopub.execute_input":"2024-09-27T01:13:09.806335Z","iopub.status.idle":"2024-09-27T01:13:09.818135Z","shell.execute_reply.started":"2024-09-27T01:13:09.806314Z","shell.execute_reply":"2024-09-27T01:13:09.817329Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"The next cell contains a simple function capable of removing artifacts such as tokens, double asterisks, or spaces which sometimes appear in outputs from large language models.\n","metadata":{}},{"cell_type":"code","source":"def clean_text(txt, EOS_TOKEN):\n    \"\"\"Clean text by removing specific tokens and redundant spaces\"\"\"\n    txt = (txt\n           .replace(EOS_TOKEN, \"\") # Replace the end-of-sentence token with an empty string\n           .replace(\"**\", \"\")      # Replace double asterisks with an empty string\n           .replace(\"<pad>\", \"\")   # Replace \"<pad>\" with an empty string\n           .replace(\"  \", \" \")     # Replace double spaces with single spaces\n          ).strip()                # Strip leading and trailing spaces from the text\n    return txt","metadata":{"execution":{"iopub.status.busy":"2024-09-27T01:13:09.819188Z","iopub.execute_input":"2024-09-27T01:13:09.819448Z","iopub.status.idle":"2024-09-27T01:13:09.832386Z","shell.execute_reply.started":"2024-09-27T01:13:09.819425Z","shell.execute_reply":"2024-09-27T01:13:09.831498Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"The following function, instead, simply adds an indefinite article to a role name, something useful to make a prompt nicer and easier to read.\n","metadata":{}},{"cell_type":"code","source":"def add_indefinite_article(role_name):\n    \"\"\"Check if a role name has a determinative adjective before it, and if not, add the correct one\"\"\"\n    \n    # Check if the first word is a determinative adjective\n    determinative_adjectives = [\"a\", \"an\", \"the\"]\n    words = role_name.split()\n    if words[0].lower() not in determinative_adjectives:\n        # Use \"a\" or \"an\" based on the first letter of the role name\n        determinative_adjective = \"an\" if words[0][0].lower() in \"aeiou\" else \"a\"\n        role_name = f\"{determinative_adjective} {role_name}\"\n\n    return role_name","metadata":{"execution":{"iopub.status.busy":"2024-09-27T01:13:09.833416Z","iopub.execute_input":"2024-09-27T01:13:09.833700Z","iopub.status.idle":"2024-09-27T01:13:09.843043Z","shell.execute_reply.started":"2024-09-27T01:13:09.833677Z","shell.execute_reply":"2024-09-27T01:13:09.842201Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"After the previous functions, mostly devoted to processing text for better readability, the next class helps first to load and initialize Gemma by quantizing it to 4-bit, reducing its memory footprint and allowing for faster responses, and then to generate text from it. <u>**Gemma** is the core of our generative functions</u>, making it a crucial element for processing information and returning it to the user in the most usable and useful form.\n","metadata":{}},{"cell_type":"markdown","source":"The `GemmaHF` class serves as a wrapper for the Transformers implementation of Gemma. Upon initialization, it sets up the model and tokenizer using the specified model name and a maximum sequence length for the tokenizer. \n\nIn short, the method `initialize_model` is designed to set up a 4-bit quantized causal language model (LLM) and tokenizer and configure them. It begins by defining the data type for computation as `float16`. Then, it creates a configuration for quantization using the `BitsAndBytesConfig` class with settings for 4-bit quantization. The function loads a pre-trained model (Gemma 2b-it in the project, but you can try also the 7b version) with the specified quantization configuration. It also loads a tokenizer with the selected device mapping and maximum sequence length settings. Finally, the method returns the initialized model and tokenizer, ready for use by our AI assistant.\n\nFinally, its `generate_text` method takes a prompt as input and generates a text using the instantiated tokenizer and model, allowing for customization of parameters such as maximum new tokens and temperature for sampling. Under the hood, it encodes the prompt, generates text based on it, decodes the output into text, and returns a list of generated text results.\n","metadata":{}},{"cell_type":"code","source":"class GemmaHF():\n    \"\"\"Wrapper for the Transformers implementation of Gemma\"\"\"\n    \n    def __init__(self, model_name, max_seq_length=2048):\n        self.model_name = model_name\n        self.max_seq_length = max_seq_length\n        \n        # Initialize the model and tokenizer\n        print(\"\\nInitializing model:\")\n        self.device = define_device()\n        self.model, self.tokenizer = self.initialize_model(self.model_name, self.device, self.max_seq_length)\n        \n    def initialize_model(self, model_name, device, max_seq_length):\n        \"\"\"Initialize a 4-bit quantized causal language model (LLM) and tokenizer with specified settings\"\"\"\n\n        # Define the data type for computation\n        compute_dtype = getattr(torch, \"float16\")\n\n        # Define the configuration for quantization\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=compute_dtype,\n        )\n\n        # Load the pre-trained model with quantization configuration\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            device_map=device,\n            quantization_config=bnb_config,\n        )\n\n        # Load the tokenizer with specified device and max_seq_length\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_name,\n            device_map=device,\n            max_seq_length=max_seq_length\n        )\n        \n        # Return the initialized model and tokenizer\n        return model, tokenizer\n    \n    def generate_text(self, prompt, max_new_tokens=2048, temperature=0.0):\n        \"\"\"Generate text using the instantiated tokenizer and model with specified settings\"\"\"\n    \n        # Encode the prompt and convert to PyTorch tensor\n        input_ids = self.tokenizer(prompt, return_tensors=\"pt\", padding=True).to(self.device)\n\n        # Determine if sampling should be performed based on temperature\n        do_sample = True if temperature > 0 else False\n\n        # Generate text based on the input prompt\n        outputs = self.model.generate(**input_ids, \n                                      max_new_tokens=max_new_tokens, \n                                      do_sample=do_sample, \n                                      temperature=temperature\n                                     )\n\n        # Decode the generated output into text\n        results = [self.tokenizer.decode(output) for output in outputs]\n\n        # Return the list of generated text results\n        return results","metadata":{"execution":{"iopub.status.busy":"2024-09-27T01:13:09.844407Z","iopub.execute_input":"2024-09-27T01:13:09.844758Z","iopub.status.idle":"2024-09-27T01:13:09.857908Z","shell.execute_reply.started":"2024-09-27T01:13:09.844729Z","shell.execute_reply":"2024-09-27T01:13:09.857027Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"And here we arrive at the core of the generative function (before we just initialized the generative engine, Gemma).","metadata":{}},{"cell_type":"markdown","source":"The `generate_summary_and_answer` function, generates an answer for a given question using context from a dataset. It embeds the input question (using the `get_embedding` function we previously saw), finds similar contexts in the dataset, extracts relevant context based on similarity indices, generates prompts for summarizing the context and providing an answer, generates summaries and answers using the a generative method from a \"model\" class, which can be a wrapper class containing Gemma implementations based on HF Transformers, Keras, Gemma C++ or any other available. Afterwards, the function cleans the generated summary and answer, and returns the cleaned answer for further processing. This function works as a sequence of steps in order to generate informative responses starting from an input question and some knowledge base data previously provided.\n","metadata":{}},{"cell_type":"markdown","source":"The two-step execution processing the information retrieved from the knowledge base is necessary because extraction based on embedded vectors sometimes returns irrelevant information. It is a problem based on the fact that embeddings are a mapping that has many facets (they are high-dimensional themselves) and that distance measures, and methods for finding what documents or text are most similar to your question, are often approximate for performance reasons resulting sometimes in unexpected retrieved results. First summarizing relevant information, a task that Gemma can execute with prowess, helps in having a shorter, more compact, and surely more relevant context to provide to the further processing by Gemma, which consists of writing an answer to your question.\n","metadata":{}},{"cell_type":"markdown","source":"In this process, temperature, the level of creativity, and the role may result in different answers and also different answering styles. I decided to rely on the \"expert data scientist\" role, but you may decide for the \"ELI5 divulgator\" or the \"verbose scholarly narrator\" (at your own risk XD).\n","metadata":{}},{"cell_type":"markdown","source":"Finally, notice the part of the generative prompt that says: \"If the context doesn't provide any relevant information, answer with <I couldn't find a good match in my knowledge base for your query, hence I answer based on my own knowledge>\". This is partly to prevent the assistant from losing its usefulness and to alert the user regarding the assistant providing peculiar answers when the question is off-topic, too difficult, or lacks sufficient information.\n","metadata":{}},{"cell_type":"code","source":"def generate_summary_and_answer(question, data, searcher, embedding_model, model,\n                                max_new_tokens=2048, temperature=0.4, role=\"expert\"):\n    \"\"\"Generate an answer for a given question using context from a dataset\"\"\"\n    \n    # Embed the input question using the provided embedding model\n    embeded_question = np.array(get_embedding(question, embedding_model)).reshape(1, -1)\n    \n    # Find similar contexts in the dataset based on the embedded question\n    neighbors, distances = searcher.search_batched(embeded_question)\n    \n    # Extract context from the dataset based on the indices of similar contexts\n    context = \" \".join([data[pos] for pos in np.ravel(neighbors)])\n    \n    # Get the end-of-sentence token from the tokenizer\n    try:\n        EOS_TOKEN = model.tokenizer.eos_token\n    except:\n        EOS_TOKEN = \"<eos>\"\n    \n    # Add a determinative adjective to the role\n    role = add_indefinite_article(role)\n    \n    # Generate a prompt for summarizing the context\n    prompt = f\"\"\"\n             Summarize this context: \"{context}\" in order to answer the question \"{question}\" as {role}\\\n             SUMMARY:\n             \"\"\".strip() + EOS_TOKEN\n    \n    # Generate a summary based on the prompt\n    results = model.generate_text(prompt, max_new_tokens, temperature)\n    \n    # Clean the generated summary\n    summary = clean_text(results[0].split(\"SUMMARY:\")[-1], EOS_TOKEN)\n\n    # Generate a prompt for providing an answer\n    prompt = f\"\"\"\n             Here is the context: {summary}\n             Using the relevant information from the context \n             and integrating it with your knowledge,\n             provide an answer as {role} to the question: {question}.\n             If the context doesn't provide\n             any relevant information answer with \n             [I couldn't find a good match in my\n             knowledge base for your question, \n             hence I answer based on my own knowledge] \\\n             ANSWER:\n             \"\"\".strip() + EOS_TOKEN\n\n    # Generate an answer based on the prompt\n    results = model.generate_text(prompt, max_new_tokens, temperature)\n    \n    # Clean the generated answer\n    answer = clean_text(results[0].split(\"ANSWER:\")[-1], EOS_TOKEN)\n\n    # Return the cleaned answer\n    return answer","metadata":{"execution":{"iopub.status.busy":"2024-09-27T01:13:09.861152Z","iopub.execute_input":"2024-09-27T01:13:09.861407Z","iopub.status.idle":"2024-09-27T01:13:09.874166Z","shell.execute_reply.started":"2024-09-27T01:13:09.861386Z","shell.execute_reply":"2024-09-27T01:13:09.873360Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# 4. Wrapping up everything","metadata":{}},{"cell_type":"markdown","source":"At this point, the next cell wraps all the functions into an `AIAssistant` class.\n","metadata":{}},{"cell_type":"markdown","source":"The `AIAssistant` class impersonates an AI assistant that interacts with users by providing answers based on a given knowledge base (basically a list of texts containing the knowledge).\n\nUpon initialization, the class loads an embedding model, indexes the knowledge base for efficient search, initializes a language model and tokenizer, and builds a searcher for similarity search using the SCANN library. The class includes functions to query the knowledge base, adjust the assistant's temperature (creativity), and define its answering style.\n\n- The `query` function generates and prints an answer to a user query by utilizing the `generate_summary_and_answer` function.\n- The `set_temperature` function allows adjusting the assistant's creativity level, while the `set_role` function defines the answering style of the AI assistant.\n\nThis class wraps all together the functionality of an AI assistant that makes good use of embeddings, powerful language models such as Gemma, and similarity search to provide informative responses to user queries based on a predefined knowledge base.\n","metadata":{}},{"cell_type":"markdown","source":"A few notes about ScaNN. ScaNN is a library developed by Google Research that offers efficient and scalable nearest neighbor search capabilities. It provides advantages over other solutions by utilizing techniques like quantization and Anisotropic Hashing, which enhance search performance.\n\nAnisotropic Hashing is a method used in hashing techniques for multimodal retrieval that involves learning projection functions to produce dimensions with varying lengths or scales. This flexibility in scaling can be beneficial for capturing complex relationships and structures in high-dimensional data, offering improved retrieval performance in scenarios where isotropic methods may not be as effective. You can read everything about this method in the paper:\n\nGuo, Ruiqi, et al. \"Accelerating large-scale inference with anisotropic vector quantization.\" International Conference on Machine Learning. PMLR, 2020. ([Paper Link](https://arxiv.org/abs/1908.10396))\n\nor by browsing the code repository at [https://github.com/google-research/google-research/tree/master/scann](https://github.com/google-research/google-research/tree/master/scann)\n\nWhat is interesting to note is that in my solution I do not use the cosine distance but simply the dot product as suggested by this paper:\n\nSteck, Harald, Chaitanya Ekanadham, and Nathan Kallus. \"Is Cosine-Similarity of Embeddings Really About Similarity?.\" arXiv preprint arXiv:2403.05440 (2024). ([Paper Link](https://arxiv.org/html/2403.05440v1))\n\nAnd it works pretty well!\n","metadata":{}},{"cell_type":"code","source":"\nclass AIAssistant():\n    \"\"\"An AI assistant that interacts with users by providing answers based on a provided knowledge base\"\"\"\n    \n    def __init__(self, gemma_model, embeddings_name=\"thenlper/gte-large\", temperature=0.4, role=\"expert\"):\n        \"\"\"Initialize the AI assistant.\"\"\"\n        # Initialize attributes\n        self.embeddings_name = embeddings_name\n        self.knowledge_base = []\n        self.temperature = temperature\n        self.role = role\n        \n        # Initialize Gemma model (it can be transformer-based or any other)\n        self.gemma_model = gemma_model\n        \n        # Load the embedding model\n        self.embedding_model = SentenceTransformer(self.embeddings_name)\n        \n    def store_knowledge_base(self, knowledge_base):\n        \"\"\"Store the knowledge base\"\"\"\n        self.knowledge_base=knowledge_base\n        \n    def learn_knowledge_base(self, knowledge_base):\n        \"\"\"Store and index the knowledge based to be used by the assistant\"\"\"\n        # Storing the knowledge base\n        self.store_knowledge_base(knowledge_base)\n        \n        # Load and index the knowledge base\n        print(\"Indexing and mapping the knowledge base:\")\n        embeddings = map2embeddings(self.knowledge_base, self.embedding_model)\n        self.embeddings = np.array(embeddings).astype(np.float32)\n        \n        # Instantiate the searcher for similarity search\n        self.index_embeddings()\n        \n    def index_embeddings(self):\n        \"\"\"Index the embeddings using ScaNN \"\"\"\n        self.searcher = (scann.scann_ops_pybind.builder(db=self.embeddings, num_neighbors=10, distance_measure=\"dot_product\")\n                 .tree(num_leaves=min(self.embeddings.shape[0] // 2, 1000), \n                       num_leaves_to_search=100, \n                       training_sample_size=self.embeddings.shape[0])\n                 .score_ah(2, anisotropic_quantization_threshold=0.2)\n                 .reorder(100)\n                 .build()\n           )\n        \n    def query(self, query):\n        \"\"\"Query the knowledge base of the AI assistant.\"\"\"\n        # Generate and print an answer to the query\n        answer = generate_summary_and_answer(query, \n                                             self.knowledge_base, \n                                             self.searcher, \n                                             self.embedding_model, \n                                             self.gemma_model,\n                                             temperature=self.temperature,\n                                             role=self.role)\n        print(answer)\n        \n    def set_temperature(self, temperature):\n        \"\"\"Set the temperature (creativity) of the AI assistant.\"\"\"\n        self.temperature = temperature\n        \n    def set_role(self, role):\n        \"\"\"Define the answering style of the AI assistant.\"\"\"\n        self.role = role\n        \n    def save_embeddings(self, filename=\"embeddings.npy\"):\n        \"\"\"Save the embeddings to disk\"\"\"\n        np.save(filename, self.embeddings)\n        \n    def load_embeddings(self, filename=\"embeddings.npy\"):\n        \"\"\"Load the embeddings from disk and index them\"\"\"\n        self.embeddings = np.load(filename)\n        # Re-instantiate the searcher\n        self.index_embeddings()","metadata":{"execution":{"iopub.status.busy":"2024-09-27T01:13:09.875164Z","iopub.execute_input":"2024-09-27T01:13:09.875406Z","iopub.status.idle":"2024-09-27T01:13:09.890411Z","shell.execute_reply.started":"2024-09-27T01:13:09.875385Z","shell.execute_reply":"2024-09-27T01:13:09.889634Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# 5. Providing the knowledge base from Wikipedia","metadata":{}},{"cell_type":"markdown","source":"In order to provide a **knowledge base** for the AI Assistant to work confidently with data science questions, I decided to retrieve some information from Wikipedia.","metadata":{}},{"cell_type":"markdown","source":"**Why Wikipedia?**\n\nActually, Wikipedia provides a vast and diverse range of information on various topics, making it a rich source for context data. Also, its structured organization allows for easy extraction and processing, also thanks to the wikipediaapi interface.\n","metadata":{}},{"cell_type":"markdown","source":"The following code, apart from the first two functions useful for cleaning the text from tags and formatting, extracts references, such as pages or other Wikipedia categories, using the `extract_wikipedia_pages` function. Then, the `get_wikipedia_pages` function takes care to crawl to all the pages and information related to some initial Wikipedia category or page.\n","metadata":{}},{"cell_type":"code","source":"# Pre-compile the regular expression pattern for better performance\nBRACES_PATTERN = re.compile(r'\\{.*?\\}|\\}')\n\ndef remove_braces_and_content(text):\n    \"\"\"Remove all occurrences of curly braces and their content from the given text\"\"\"\n    return BRACES_PATTERN.sub('', text)\n\ndef clean_string(input_string):\n    \"\"\"Clean the input string.\"\"\"\n    \n    # Remove extra spaces by splitting the string by spaces and joining back together\n    cleaned_string = ' '.join(input_string.split())\n    \n    # Remove consecutive carriage return characters until there are no more consecutive occurrences\n    cleaned_string = re.sub(r'\\r+', '\\r', cleaned_string)\n    \n    # Remove all occurrences of curly braces and their content from the cleaned string\n    cleaned_string = remove_braces_and_content(cleaned_string)\n    \n    # Return the cleaned string\n    return cleaned_string","metadata":{"execution":{"iopub.status.busy":"2024-09-27T01:13:09.891557Z","iopub.execute_input":"2024-09-27T01:13:09.891871Z","iopub.status.idle":"2024-09-27T01:13:09.905080Z","shell.execute_reply.started":"2024-09-27T01:13:09.891841Z","shell.execute_reply":"2024-09-27T01:13:09.904224Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def extract_wikipedia_pages(wiki_wiki, category_name):\n    \"\"\"Extract all references from a category on Wikipedia\"\"\"\n    \n    # Get the Wikipedia page corresponding to the provided category name\n    category = wiki_wiki.page(\"Category:\" + category_name)\n    \n    # Initialize an empty list to store page titles\n    pages = []\n    \n    # Check if the category exists\n    if category.exists():\n        # Iterate through each article in the category and append its title to the list\n        for article in category.categorymembers.values():\n            pages.append(article.title)\n    \n    # Return the list of page titles\n    return pages","metadata":{"execution":{"iopub.status.busy":"2024-09-27T01:13:09.906268Z","iopub.execute_input":"2024-09-27T01:13:09.906677Z","iopub.status.idle":"2024-09-27T01:13:09.919449Z","shell.execute_reply.started":"2024-09-27T01:13:09.906649Z","shell.execute_reply":"2024-09-27T01:13:09.918650Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def get_wikipedia_pages(categories):\n    \"\"\"Retrieve Wikipedia pages from a list of categories and extract their content\"\"\"\n    \n    # Create a Wikipedia object\n    wiki_wiki = wikipediaapi.Wikipedia('Gemma AI Assistant (gemma@example.com)', 'en')\n    \n    # Initialize lists to store explored categories and Wikipedia pages\n    explored_categories = []\n    wikipedia_pages = []\n\n    # Iterate through each category\n    print(\"- Processing Wikipedia categories:\")\n    for category_name in categories:\n        print(f\"\\tExploring {category_name} on Wikipedia\")\n        \n        # Get the Wikipedia page corresponding to the category\n        category = wiki_wiki.page(\"Category:\" + category_name)\n        \n        # Extract Wikipedia pages from the category and extend the list\n        wikipedia_pages.extend(extract_wikipedia_pages(wiki_wiki, category_name))\n        \n        # Add the explored category to the list\n        explored_categories.append(category_name)\n\n    # Extract subcategories and remove duplicate categories\n    categories_to_explore = [item.replace(\"Category:\", \"\") for item in wikipedia_pages if \"Category:\" in item]\n    wikipedia_pages = list(set([item for item in wikipedia_pages if \"Category:\" not in item]))\n    \n    # Explore subcategories recursively\n    while categories_to_explore:\n        category_name = categories_to_explore.pop()\n        print(f\"\\tExploring {category_name} on Wikipedia\")\n        \n        # Extract more references from the subcategory\n        more_refs = extract_wikipedia_pages(wiki_wiki, category_name)\n\n        # Iterate through the references\n        for ref in more_refs:\n            # Check if the reference is a category\n            if \"Category:\" in ref:\n                new_category = ref.replace(\"Category:\", \"\")\n                # Add the new category to the explored categories list\n                if new_category not in explored_categories:\n                    explored_categories.append(new_category)\n            else:\n                # Add the reference to the Wikipedia pages list\n                if ref not in wikipedia_pages:\n                    wikipedia_pages.append(ref)\n\n    # Initialize a list to store extracted texts\n    extracted_texts = []\n    \n    # Iterate through each Wikipedia page\n    print(\"- Processing Wikipedia pages:\")\n    for page_title in tqdm(wikipedia_pages):\n        try:\n            # Make a request to the Wikipedia page\n            page = wiki_wiki.page(page_title)\n\n            # Check if the page summary does not contain certain keywords\n            if \"Biden\" not in page.summary and \"Trump\" not in page.summary:\n                # Append the page title and summary to the extracted texts list\n                if len(page.summary) > len(page.title):\n                    extracted_texts.append(page.title + \" : \" + clean_string(page.summary))\n\n                # Iterate through the sections in the page\n                for section in page.sections:\n                    # Append the page title and section text to the extracted texts list\n                    if len(section.text) > len(page.title):\n                        extracted_texts.append(page.title + \" : \" + clean_string(section.text))\n                        \n        except Exception as e:\n            print(f\"Error processing page {page.title}: {e}\")\n                    \n    # Return the extracted texts\n    return extracted_texts","metadata":{"execution":{"iopub.status.busy":"2024-09-27T01:13:09.920565Z","iopub.execute_input":"2024-09-27T01:13:09.920856Z","iopub.status.idle":"2024-09-27T01:13:09.933791Z","shell.execute_reply.started":"2024-09-27T01:13:09.920834Z","shell.execute_reply":"2024-09-27T01:13:09.932917Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"To develop an AI assistant capable of answering questions about data science, I've chosen to begin with topics such as machine learning, data science, statistics, and deep learning artificial intelligence. As evident from the output, the range of topics it covers is truly impressive, even for a seasoned data scientist!\n","metadata":{}},{"cell_type":"code","source":"categories = [\"Machine_learning\", \"Data_science\", \"Statistics\", \"Deep_learning\", \"Artificial_intelligence\"]\nextracted_texts = get_wikipedia_pages(categories)\nprint(\"Found\", len(extracted_texts), \"Wikipedia pages\")","metadata":{"execution":{"iopub.status.busy":"2024-09-27T01:13:09.934874Z","iopub.execute_input":"2024-09-27T01:13:09.935138Z","iopub.status.idle":"2024-09-27T01:20:42.586084Z","shell.execute_reply.started":"2024-09-27T01:13:09.935117Z","shell.execute_reply":"2024-09-27T01:20:42.585193Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"- Processing Wikipedia categories:\n\tExploring Machine_learning on Wikipedia\n\tExploring Data_science on Wikipedia\n\tExploring Statistics on Wikipedia\n\tExploring Deep_learning on Wikipedia\n\tExploring Artificial_intelligence on Wikipedia\n\tExploring Artificial intelligence stubs on Wikipedia\n\tExploring Works created using artificial intelligence on Wikipedia\n\tExploring Turing tests on Wikipedia\n\tExploring AI safety on Wikipedia\n\tExploring Rule engines on Wikipedia\n\tExploring Regulation of artificial intelligence on Wikipedia\n\tExploring Problems in artificial intelligence on Wikipedia\n\tExploring Philosophy of artificial intelligence on Wikipedia\n\tExploring Open-source artificial intelligence on Wikipedia\n\tExploring Neural networks on Wikipedia\n\tExploring Multi-agent systems on Wikipedia\n\tExploring Machine learning on Wikipedia\n\tExploring Artificial intelligence laboratories on Wikipedia\n\tExploring Knowledge representation on Wikipedia\n\tExploring History of artificial intelligence on Wikipedia\n\tExploring Generative artificial intelligence on Wikipedia\n\tExploring Fuzzy logic on Wikipedia\n\tExploring Fiction about artificial intelligence on Wikipedia\n\tExploring Existential risk from artificial general intelligence on Wikipedia\n\tExploring Evolutionary computation on Wikipedia\n\tExploring Distributed artificial intelligence on Wikipedia\n\tExploring Artificial intelligence conferences on Wikipedia\n\tExploring Computer vision on Wikipedia\n\tExploring Artificial intelligence companies on Wikipedia\n\tExploring Cognitive architecture on Wikipedia\n\tExploring Automated reasoning on Wikipedia\n\tExploring Artificial intelligence associations on Wikipedia\n\tExploring Artificial intelligence templates on Wikipedia\n\tExploring Artificial intelligence publications on Wikipedia\n\tExploring Artificial intelligence people on Wikipedia\n\tExploring Artificial intelligence entertainment on Wikipedia\n\tExploring Artificial intelligence competitions on Wikipedia\n\tExploring Artificial intelligence art on Wikipedia\n\tExploring Artificial immune systems on Wikipedia\n\tExploring Argument technology on Wikipedia\n\tExploring Applications of artificial intelligence on Wikipedia\n\tExploring Ambient intelligence on Wikipedia\n\tExploring AI software on Wikipedia\n\tExploring AI accelerators on Wikipedia\n\tExploring Affective computing on Wikipedia\n\tExploring Text-to-video generation on Wikipedia\n\tExploring Text-to-image generation on Wikipedia\n\tExploring Google DeepMind on Wikipedia\n\tExploring Deepfakes on Wikipedia\n\tExploring Deep learning software on Wikipedia\n\tExploring Statistics stubs on Wikipedia\n\tExploring Statistical concepts on Wikipedia\n\tExploring Statistical software on Wikipedia\n\tExploring Statistical methods on Wikipedia\n\tExploring Statistical data on Wikipedia\n\tExploring Subfields of statistics on Wikipedia\n\tExploring Statistics profession and organizations on Wikipedia\n\tExploring Statistics-related lists on Wikipedia\n\tExploring Statisticians on Wikipedia\n\tExploring Data scientists on Wikipedia\n\tExploring Unsupervised learning on Wikipedia\n\tExploring Support vector machines on Wikipedia\n\tExploring Supervised learning on Wikipedia\n\tExploring Structured prediction on Wikipedia\n\tExploring Statistical natural language processing on Wikipedia\n\tExploring Semisupervised learning on Wikipedia\n\tExploring Natural language processing researchers on Wikipedia\n\tExploring Machine learning researchers on Wikipedia\n\tExploring Reinforcement learning on Wikipedia\n\tExploring Ontology learning (computer science) on Wikipedia\n\tExploring Markov models on Wikipedia\n\tExploring Machine learning task on Wikipedia\n\tExploring Machine learning algorithms on Wikipedia\n\tExploring Loss functions on Wikipedia\n\tExploring Log-linear models on Wikipedia\n\tExploring Learning in computer vision on Wikipedia\n\tExploring Latent variable models on Wikipedia\n\tExploring Kernel methods for machine learning on Wikipedia\n\tExploring Inductive logic programming on Wikipedia\n\tExploring Genetic programming on Wikipedia\n\tExploring Evolutionary algorithms on Wikipedia\n\tExploring Ensemble learning on Wikipedia\n\tExploring Dimension reduction on Wikipedia\n\tExploring Datasets in machine learning on Wikipedia\n\tExploring Data mining and machine learning software on Wikipedia\n\tExploring Signal processing conferences on Wikipedia\n\tExploring Artificial intelligence conferences on Wikipedia\n\tExploring Computational learning theory on Wikipedia\n\tExploring Cluster analysis on Wikipedia\n\tExploring Classification algorithms on Wikipedia\n\tExploring Blockmodeling on Wikipedia\n\tExploring Bayesian networks on Wikipedia\n\tExploring Artificial neural networks on Wikipedia\n\tExploring Applied machine learning on Wikipedia\n- Processing Wikipedia pages:\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3320/3320 [07:11<00:00,  7.69it/s]","output_type":"stream"},{"name":"stdout","text":"Found 15522 Wikipedia pages\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"As a last step, the extracted knowledge base is saved to disk for later usage","metadata":{}},{"cell_type":"code","source":"wikipedia_data_science_kb = pd.DataFrame(extracted_texts, columns=[\"wikipedia_text\"])\nwikipedia_data_science_kb.to_csv(\"wikipedia_data_science_kb.csv\", index=False)\nwikipedia_data_science_kb.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-27T01:20:42.587173Z","iopub.execute_input":"2024-09-27T01:20:42.587472Z","iopub.status.idle":"2024-09-27T01:20:43.184282Z","shell.execute_reply.started":"2024-09-27T01:20:42.587428Z","shell.execute_reply":"2024-09-27T01:20:43.183415Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"                                      wikipedia_text\n0  Intelligent decision support system : An intel...\n1  Intelligent decision support system : Turban, ...\n2  Personoid : Personoid is the concept coined by...\n3  Personoid : Welt am Draht (1973) The Thirteent...\n4  Personoid : Android Humanoid Intelligence Arti...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>wikipedia_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Intelligent decision support system : An intel...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Intelligent decision support system : Turban, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Personoid : Personoid is the concept coined by...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Personoid : Welt am Draht (1973) The Thirteent...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Personoid : Android Humanoid Intelligence Arti...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# 6. A test run","metadata":{}},{"cell_type":"markdown","source":"We are now ready to test our AI assistant!\n\nWe instantiate it using the Gemma 2b-it and the gte-large embeddings and provide the extracts from Wikipedia as a knowledge base.\n\nThe Generate Text Embedding (gte) model is a variant of the BERT model developed by Alibaba DAMO Academy. This embedding model is available in three versions (large, base, small) and is specifically designed for English text processing. In comparisons with other embedding models, the gte-large variant demonstrates superior performance in retrieval tasks, but it also needs more storage space for embedding vectors compared to competitors (we do not worry much about that because ScaNN is quite fast for this application).\n\nThe instantiation will take a short while, then you can ask a few questions to the AI assistant.\n\n","metadata":{}},{"cell_type":"code","source":"# Initialize the name of the embeddings and model\nembeddings_name = \"thenlper/gte-large\"\nmodel_name = \"/kaggle/input/gemma/transformers/2b-it/1\"\n\n# Create an instance of AIAssistant with specified parameters\ngemma_ai_assistant = AIAssistant(gemma_model=GemmaHF(model_name), embeddings_name=embeddings_name)\n\n# Map the intended knowledge base to embeddings and index it\ngemma_ai_assistant.learn_knowledge_base(knowledge_base=extracted_texts)\n\n# Save the embeddings to disk (for later use)\ngemma_ai_assistant.save_embeddings()\n\n# Set the temperature (creativity) of the AI assistant and set the role\ngemma_ai_assistant.set_temperature(0.0)\ngemma_ai_assistant.set_role(\"data science expert whose explanations are useful, clear and complete\")","metadata":{"execution":{"iopub.status.busy":"2024-09-27T01:20:43.185630Z","iopub.execute_input":"2024-09-27T01:20:43.186249Z","iopub.status.idle":"2024-09-27T01:32:29.158652Z","shell.execute_reply.started":"2024-09-27T01:20:43.186221Z","shell.execute_reply":"2024-09-27T01:32:29.157781Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"\nInitializing model:\nPyTorch version: 2.1.2 -- using cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6ed95c10c3c4c8488636133792d784a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"231205afbe9d481cb94e3dc980c4de67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/67.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"830117d2cd414327a24276e10df471d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73de6b1276a2421bb51e5384b350fd71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/619 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fae066ae29944a66aaed98a363d483b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/670M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e80e4e34b87439d92000bb6b5fc58ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fd3017fd409446c8d47bc425ee7116b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"981d0ed796c14b17a345ecfb2e05b27a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e2958fabd964f11b4821ffb3b9e5598"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66f528a20f72439298374f21d2930be9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5e46455225c418abfe8c88140fd8217"}},"metadata":{}},{"name":"stdout","text":"Indexing and mapping the knowledge base:\nMapping 15522 pieces of information\n","output_type":"stream"},{"text":" 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 8777/15522 [06:04<04:30, 24.95it/s]IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n","name":"stderr","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's start with a warm-up question: \"What is the difference between data science, machine learning, and artificial intelligence?\"\n","metadata":{}},{"cell_type":"code","source":"gemma_ai_assistant.query(\"What is the difference between data science, machine learning, and artificial intelligence?\")","metadata":{"execution":{"iopub.status.busy":"2024-09-27T01:32:29.160071Z","iopub.execute_input":"2024-09-27T01:32:29.160903Z","iopub.status.idle":"2024-09-27T01:32:48.647581Z","shell.execute_reply.started":"2024-09-27T01:32:29.160875Z","shell.execute_reply":"2024-09-27T01:32:48.646633Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Sure, here's an answer to the question:\n\nThe difference between data science, machine learning, and artificial intelligence is as follows:\n\nData Science is an interdisciplinary field that combines statistical analysis, computational methods, and machine learning to extract insights from data and make data-driven decisions.\n\nMachine Learning is a subfield of soft computing that focuses on the development and study of statistical algorithms that can learn from data and generalize to unseen data.\n\nArtificial Intelligence is a broader field that encompasses the study of intelligent agents, which are systems that can learn and reason.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now a more complicated question, that you may also encounter in a data science interview!\n","metadata":{}},{"cell_type":"code","source":"gemma_ai_assistant.query(\"Explain how linear regression works\")","metadata":{"execution":{"iopub.status.busy":"2024-09-27T01:32:48.648839Z","iopub.execute_input":"2024-09-27T01:32:48.649138Z","iopub.status.idle":"2024-09-27T01:33:30.834054Z","shell.execute_reply.started":"2024-09-27T01:32:48.649112Z","shell.execute_reply":"2024-09-27T01:33:30.833109Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Sure, here's an explanation of how linear regression works:\n\nLinear regression is a statistical method used to predict a dependent variable based on one or more independent variables. It is a simple but powerful technique that can be used to model a wide range of relationships between the dependent and independent variables.\n\nThe basic idea behind linear regression is that the dependent variable can be expressed as a linear function of the independent variables. This means that the dependent variable can be represented by a straight line.\n\nThe linear predictor function is a function that relates the independent variables to the dependent variable. The weights of the independent variables are chosen to optimize the relationship between the prediction and the criterion.\n\nThe process of linear regression involves the following steps:\n\n1. Define the linear predictor function.\n2. Choose a regression method.\n3. Fit the model to the data.\n4. Evaluate the model's performance.\n\nThe linear predictor function is a function that expresses the relationship between the dependent and independent variables in a linear equation. The coefficients of the independent variables in this equation represent the weights of the variables in the linear predictor function.\n\nThe regression method is chosen based on the data and the research question. There are many different regression methods available, each with its own strengths and weaknesses.\n\nOnce the linear predictor function and regression method have been chosen, the model is fitted to the data. This process involves finding the values of the coefficients that minimize the sum of the squared errors between the predicted values and the actual values.\n\nThe model's performance is then evaluated to determine how well it fits the data. There are many different metrics that can be used to evaluate model performance, including mean squared error (MSE), root mean squared error (RMSE), and adjusted R-squared.\n\nIf the model's performance is satisfactory, it can be used to make predictions about the dependent variable based on the independent variables.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's ask for more complex methods and algorithms, such as decision trees.\n","metadata":{}},{"cell_type":"code","source":"gemma_ai_assistant.query(\"What are decision trees, and how do they work in machine learning?\")","metadata":{"execution":{"iopub.status.busy":"2024-09-27T01:33:30.835294Z","iopub.execute_input":"2024-09-27T01:33:30.835672Z","iopub.status.idle":"2024-09-27T01:34:01.003619Z","shell.execute_reply.started":"2024-09-27T01:33:30.835636Z","shell.execute_reply":"2024-09-27T01:34:01.002697Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Sure, here's a summary of the context:\n\nDecision Tree Learning is a supervised machine learning approach used to predict the value of a target variable based on several input variables.\n\nDecision Tree Model is a tree-like structure consisting of nodes connected by arcs. Each node represents a feature, and each arc represents a possible value of the target variable. The leaves of the tree represent the different classes or probabilities of the target variable.\n\nTypes of Decision Tree Learning include classification tree analysis and regression tree analysis.\n\nDecision Tree Pruning is a data compression technique that reduces the size of a decision tree without reducing its predictive accuracy.\n\nCommon Decision Tree Algorithms include ID3, C4.5, CART, OC1, and Random Forest.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The next question, about cross-validation, is a return to fundamentals.","metadata":{}},{"cell_type":"code","source":"gemma_ai_assistant.query(\"What is cross-validation, and why is it used in machine learning?\")","metadata":{"execution":{"iopub.status.busy":"2024-09-27T01:34:01.004762Z","iopub.execute_input":"2024-09-27T01:34:01.005037Z","iopub.status.idle":"2024-09-27T01:34:26.229593Z","shell.execute_reply.started":"2024-09-27T01:34:01.005013Z","shell.execute_reply":"2024-09-27T01:34:26.228744Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Sure, here's an answer to your question:\n\nCross-validation is a statistical technique used in machine learning to assess how well a predictive model will generalize to an independent data set. It involves resampling and splitting the original data set into multiple subsets, training a model on each subset and validating its performance on a separate subset. This process is repeated multiple times to reduce variability and obtain a more accurate estimate of the model's predictive performance.\n\nCross-validation is used in machine learning because it helps to reduce overfitting, which is when a model is too closely fit to the training data and cannot generalize well to new data. Overfitting can lead to poor performance on unseen data, even if the model was highly accurate on the training data. Cross-validation helps to mitigate overfitting by training the model on a subset of the data and validating its performance on a different subset. This process helps to ensure that the model is not too heavily biased towards the training data and can perform well on unseen data.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Finally, a tricky question on regularization. How will the AI Assistant handle it?","metadata":{}},{"cell_type":"code","source":"gemma_ai_assistant.query(\"Explain the concept of regularization and its importance in preventing overfitting in machine learning models\")","metadata":{"execution":{"iopub.status.busy":"2024-09-27T01:34:26.230630Z","iopub.execute_input":"2024-09-27T01:34:26.230899Z","iopub.status.idle":"2024-09-27T01:35:02.024117Z","shell.execute_reply.started":"2024-09-27T01:34:26.230875Z","shell.execute_reply":"2024-09-27T01:35:02.023246Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Regularization is a process of introducing additional information to solve an ill-posed problem or to prevent overfitting. It can be achieved by restricting the hypothesis space, penalizing complex functions, or adding constraints to the minimization function.\n\nRegularization helps to prevent overfitting by:\n\n* Reducing the complexity of the function.\n* Preventing the function from becoming too complex, which can lead to overfitting.\n* Making the function more robust to noise in the data.\n\nRegularization techniques can be divided into two main categories:\n\n* Model comparison: This technique involves comparing the performance of different models on the same dataset. The model with the best performance is selected.\n* Cross-validation: This technique involves splitting the dataset into multiple folds, training the model on each fold and evaluating it on the remaining folds. The model with the best performance on the test set is selected.\n\nRegularization is an important technique for preventing overfitting in machine learning models. By introducing additional information into the learning process, regularization can help to create models that are more robust and accurate.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 7. Preparing for deploying the model","metadata":{}},{"cell_type":"markdown","source":"In order to deploy the model, you just need the files that we saved and a copy of the functions and classes that we used in this notebook. The procedure is the same, although you don't need to embed again the knowledge base, you just reload the previously calcualted embeddings. However, the previously seen code works speedly with a GPU available.","metadata":{}},{"cell_type":"markdown","source":"If you actually have access only with CPU machine at the inference phase, you can leverage the C++ version of Gemma, which, based on 8-bit switched floating point compressed weights, can offer an adequate speed of text processing. I take the compiled version from another notebook (see https://www.kaggle.com/code/lucamassaron/gemma-cpp for more details on the compiling procedure) from where I copy the Gemma C++ executable. I also allow the exacutable to be executed and install Google SentencePiece, whose libraries are necessary for the executable to work (in particular the libsentencepiece.so library).","metadata":{}},{"cell_type":"code","source":"!cp -r /kaggle/input/gemma-cpp/gemma_cpp /kaggle/working/gemma_cpp # Copy compiled Gemma C++\n!chmod +x ./gemma_cpp/gemma # Make Gemma C++ executable\n!mamba install -q -c conda-forge sentencepiece -y # Install Google SentencePiece (https://github.com/google/sentencepiece)","metadata":{"execution":{"iopub.status.busy":"2024-09-27T01:35:02.025395Z","iopub.execute_input":"2024-09-27T01:35:02.025792Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/497deca9.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/09cdf8bf.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/c6f2354e.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/86b0f08d.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/c9ddbd6b.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/b121c3e7.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/47929eba.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/3e39a7aa.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/2ce54b42.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/4ea078d6.json\" was modified by another program\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The following Python code defines a class named `GemmaCPP`, which works as a wrapper for interacting with the C++ implementation of Gemma (https://github.com/google/gemma.cpp). \n\nThe class has an initializer method that takes four parameters: `gemma_cpp`, `tokenizer`, `compressed_weights`, and `model`. These parameters are used to initialize attributes of the class instance with the same names which will later serve for interacting with the commands for the C++ compiled Gemma. Additionally, the class contains a method named `generate_text`, which takes a prompt as input along with optional args and kwargs (for compatibility with other Gemma implementations). Within this method, a shell command is constructed using the provided prompt and other parameters, formatted appropriately to be executed with the Gemma C++ executable. \n\nThe `subprocess.Popen` function is then called to execute the shell command, capturing the standard output (stdout) and standard error (stderr) streams. The stdout data is decoded from bytes to a string, and if there is any error message in stderr, it is printed out. Finally, the method returns the output text wrapped in a list. This code facilitates the generation of text using Gemma's C++ implementation via Python, allowing the integration between the two languages.","metadata":{}},{"cell_type":"code","source":"import subprocess\nimport sys\nimport re\n\nclass GemmaCPP():\n    \"\"\"Wrapper for the C++ implementation of Gemma\"\"\"\n    \n    def __init__(self, gemma_cpp, tokenizer, compressed_weights, model):\n        self.gemma_cpp = gemma_cpp\n        self.tokenizer = tokenizer\n        self.compressed_weights = compressed_weights\n        self.model = model\n        \n    def eliminate_long_dots(self, input_string):\n        \"\"\"Eliminate long sequences of dots from the input string\"\"\"\n        # Define a regular expression pattern to match sequences of 2 or more dots\n        pattern = r'\\.{2,}'\n\n        # Replace all occurrences of the pattern with a space\n        output_string = re.sub(pattern, ' ', input_string)\n\n        return output_string.strip()\n    \n    def beautify_string(self, input_string):\n        \"\"\"Clean the input string by removing non-letter characters at the beginning\n           and isolated letters at the end after multiple spaces\"\"\"\n        # Remove non-letter characters at the beginning of the string\n        output_string = re.sub(r'^[^a-zA-Z]+', '', input_string.strip())\n\n        # Remove isolated letters at the end of the output string after multiple spaces\n        output_string = re.sub(r'\\s{3,}(.+)\\Z', '', output_string.strip())\n\n        return output_string\n        \n    def generate_text(self, prompt, *args, **kwargs):\n        \"\"\"Generate text using the cpp tokenizer and model\"\"\"\n\n        # Define the shell command\n        prompt = prompt.replace('\"', '').replace(\"'\", \"\")\n        shell_command = f'echo \"{prompt}\" | {gemma_cpp} -- --tokenizer {tokenizer} --compressed_weights {compressed_weights} --model {model} --verbosity 0'\n\n        # Execute the shell command and redirect stdout to the Python script's stdout\n        process = subprocess.Popen(shell_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n\n        output_text = \"\"\n        reading_block = \"[ Reading prompt ]\"\n        \n        # Communicate with the process and capture stdout \n        for k, char in enumerate( iter(lambda: process.stdout.read(1), b'') ):\n            single_char = char.decode(sys.stdout.encoding)\n            output_text += single_char\n            if len(output_text) % 20 == 0:\n                count_reading_blocks = output_text.count(reading_block)\n                if count_reading_blocks > 1:\n                    break\n                    \n        # Remove long sequences of dots and the reading block, beautify the string\n        output_text = output_text.replace(reading_block, \"\")\n        output_text = self.eliminate_long_dots(output_text)\n        output_text = self.beautify_string(output_text)\n        output_text = prompt + output_text\n        \n        # Return output text\n        return [output_text]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that everything is ready, I can reinstantiate a Gemma AI Assistant based on Gemma C++ and the knowledge base previously extractred and processed.","metadata":{}},{"cell_type":"code","source":"embeddings_name = \"thenlper/gte-large\"\ngemma_cpp = \"./gemma_cpp/gemma\"\ntokenizer = \"/kaggle/input/gemma/gemmacpp/2b-it-sfp/1/tokenizer.spm\"\ncompressed_weights = \"/kaggle/input/gemma/gemmacpp/2b-it-sfp/1/2b-it-sfp.sbs\"\nmodel = \"2b-it\"\n\n# Create an instance of the class AIAssistant based on Gemma C++\ngemma_ai_assistant = AIAssistant(\n    gemma_model=GemmaCPP(gemma_cpp, tokenizer, compressed_weights, model),\n    embeddings_name=embeddings_name\n)\n\n# Loading the previously prepared knowledge base and embeddings\nwikipedia_data_science_kb = pd.read_csv(\"wikipedia_data_science_kb.csv\")\nknowledge_base = wikipedia_data_science_kb.wikipedia_text.tolist()\n\n# Uploading the knowledge base and embeddings to the AI assistant\ngemma_ai_assistant.store_knowledge_base(knowledge_base=knowledge_base)\ngemma_ai_assistant.load_embeddings(filename=\"embeddings.npy\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try a new query on machine learning topics and see how it takes to get an answer when only CPUs (Kaggle Notebooks have 4 cores) are working:","metadata":{}},{"cell_type":"code","source":"gemma_ai_assistant.query(\"In short, what are the key differences between gradient boosting and random forests?\")","metadata":{"execution":{"iopub.status.idle":"2024-09-27T01:44:16.501620Z","shell.execute_reply.started":"2024-09-27T01:36:51.524129Z","shell.execute_reply":"2024-09-27T01:44:16.499605Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Gradient Boosting:\n\n* Uses a sequential ensemble of weak learners to iteratively improve the overall model.\n* Each weak learner is trained on a subset of the training data and makes a local decision.\n* The weak learners are then combined in a weighted manner to form the final model.\n* Gradient boosting is easy to interpret and can be used to create complex models.\n\nRandom Forests:\n\n* Uses an ensemble of decision trees to iteratively improve the overall model.\n* Each decision tree is trained on a subset of the training data and makes a local decision.\n* The decision trees are then combined in a way that reduces the variance of the final model.\n* Random forests are more robust to overfitting than gradient boosting.\n\nHere are some of the key differences between gradient boosting and random forests:\n\n| Feature | Gradient Boosting | Random Forests |\n|---|---|---|\n| Training process | Iterative, weak learners are trained on a subset of the training data | Iterative, decision trees are trained on a subset of the training data |\n| Model combination | Weighted combination of weak learners | Averaging of the predictions from the decision trees |\n| Interpretability | Easy to interpret | Less easy to interpret |\n| Robustness to overfitting | Less robust | More robust |\n\nIn general, gradient boosting is a good choice for problems where you want an easy-to-interpret model that can be used for both prediction and classification. Random forests are a good choice for problems where you want a robust model that is less likely to overfit.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 8. Conclusions","metadata":{}},{"cell_type":"markdown","source":"It seems that the AI Assistant is working fine and promptly answering questions in a correct and usable way. Using the same approach, the same code could also be used for other tasks of this competition such as:\n\n- Answering common questions about the Python programming language\n- Explaining or teaching concepts from Kaggle competition solution write-ups\n- Answering common questions about the Kaggle platform\n\nAll you need is to prepare the context data by extraction from a website, a dataset, or other sources such as the meta-Kaggle meta.\n\nEnjoy your new AI assistant powered by Gemma 2b-it :-)\n","metadata":{}}]}